5.3. APPROACH
of the sampling process. As this style is the same for all pictures in the batch D, pictures are correlated.
5.3.2 Baseline: Coupled PLSA
While the last section introduced the fundamental structure of the proposed model — with tags and visual words sampled from style-dependent distributions — we now discuss several concrete realizations of this framework making different use of style information.
The first version ignores style information completely: the topic distributions P{t|z, s) and P(v|z,s) are replaced with simpler equivalents P(t\z) and P(v\z), such that the style variable s does not have any influence and can be dropped:
# (5.2)
This model corresponds to the PLSA-words image annotation method proposed by Monay and Gatica-Perez [MGP04]. Like most standard approaches, it treats each image individually. A graphical illustration of the sampling process can be found in Figure 5.3(a).
In the following, learning and inference with this model are briefly outlined, which are both based on a maximization of the overall data likelihood (using the terms from Equation (5.2)), where n(.,d) denotes the number of occurrences of a specific tag or visual word in image d:
# (5.3)
Learning The topic posteriors P{z\d) and topic vectors P(v|z), P(t|z) are learned from a set of annotated training images D. For standard PLSA models, such learning is done by maximizing the overall data likelihood (Equation (5.3)), whereas optimization is carried out using Expectation Maximization (EM) [DLR77] or variants [Hof01]. Iteratively, two steps are applied: first, in the "E"-step, the posteriors for latent variables (i.e. the topic that each word is sampled from) are estimated. In the subsequent "M"-step, the expected log-likelihood of the training data with respect to these posteriors is maximized, resulting in updates for
108
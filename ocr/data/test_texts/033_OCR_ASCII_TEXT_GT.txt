2.3. HMM APPROACH
43
determining the expected value of the complete data log-likelihood # as following:
#
This allows us to iteratively improve upon our model # by finding the model # with maximal complete data log-likelihood:
#
Then we can re-evaluate the expectation of the complete-data log likelihood # using the newly generated model and repeat the process until convergence. As each iteration increases the complete-data log likelihood the procedure is guaranteed to converge to a local maximum of the likelihood function [Bil98].
In the case of Gaussian mixtures the probability density of given data # under our model # can be expressed as
# 
where the component weights # sum up to one.
The model's log-likelihood function on the incomplete data X is given by:
#
As this expression is hard to optimize we introduce unobserved data # that denotes the generating component for each sample in this case, i.e. # if sample # has been generated by component k from the mixture. This leads to the complete-data log-likelihood:
#